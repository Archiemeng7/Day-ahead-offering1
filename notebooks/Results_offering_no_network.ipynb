{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f3a7eadf",
   "metadata": {},
   "source": [
    "# Results â€” offering_no_network\n",
    "\n",
    "This notebook summarizes generated datasets, instances, and (if available) trained model / evaluation artifacts for **`offering_no_network`** in the Neur2RO codebase.\n",
    "\n",
    "It is written to be **robust**:\n",
    "- Works even if some result files are missing (it will skip those sections).\n",
    "- Supports MATLAB `.mat` files saved in **v7.3** (HDF5) via `h5py` fallback.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "054c7948",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Imports / setup ---\n",
    "import os, glob, pickle, math, re\n",
    "from dataclasses import dataclass\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display\n",
    "\n",
    "# Optional dependencies\n",
    "try:\n",
    "    import h5py  # for MATLAB v7.3 .mat\n",
    "except Exception:\n",
    "    h5py = None\n",
    "\n",
    "# Make plots render a bit larger by default\n",
    "plt.rcParams[\"figure.figsize\"] = (10, 4)\n",
    "plt.rcParams[\"axes.grid\"] = True\n",
    "\n",
    "# Project root (adjust if needed)\n",
    "PROJECT_ROOT = os.getcwd()\n",
    "\n",
    "DATA_ROOT = os.path.join(PROJECT_ROOT, \"data\")\n",
    "PROB_NAME  = \"offering_no_network\"\n",
    "PROB_DIR   = os.path.join(DATA_ROOT, PROB_NAME)\n",
    "\n",
    "print(\"PROJECT_ROOT:\", PROJECT_ROOT)\n",
    "print(\"PROB_DIR:\", PROB_DIR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "967a0b78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Helpers ---\n",
    "def latest_file(pattern: str):\n",
    "    \"\"\"Return latest file by mtime matching a glob pattern, or None.\"\"\"\n",
    "    files = glob.glob(pattern)\n",
    "    if not files:\n",
    "        return None\n",
    "    files.sort(key=lambda p: os.path.getmtime(p), reverse=True)\n",
    "    return files[0]\n",
    "\n",
    "def load_pkl(path: str):\n",
    "    with open(path, \"rb\") as f:\n",
    "        return pickle.load(f)\n",
    "\n",
    "def try_load_price_matrix(mat_path: str, var_candidates=(\"price_matrix_revised_100\", \"price_matrix\")):\n",
    "    \"\"\"Load a 24xS price matrix from a .mat. Supports v7.3 via h5py fallback.\"\"\"\n",
    "    # 1) scipy.io.loadmat (MATLAB v5/v7 non-HDF)\n",
    "    try:\n",
    "        from scipy.io import loadmat\n",
    "        try:\n",
    "            mat = loadmat(mat_path)\n",
    "            for var in var_candidates:\n",
    "                if var in mat:\n",
    "                    arr = np.asarray(mat[var], dtype=float)\n",
    "                    return arr\n",
    "            # fallback: any 2D 24xS\n",
    "            for k, v in mat.items():\n",
    "                if k.startswith(\"__\"):\n",
    "                    continue\n",
    "                if isinstance(v, np.ndarray) and v.ndim == 2 and v.shape[0] == 24:\n",
    "                    return np.asarray(v, dtype=float)\n",
    "            raise KeyError(f\"No variable in {var_candidates} and no 24xS array found.\")\n",
    "        except NotImplementedError:\n",
    "            # MATLAB v7.3 -> fall through to h5py\n",
    "            pass\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    # 2) h5py fallback (MATLAB v7.3)\n",
    "    if h5py is None:\n",
    "        raise RuntimeError(\"MATLAB file looks like v7.3 (HDF5), but h5py is not available.\")\n",
    "\n",
    "    with h5py.File(mat_path, \"r\") as f:\n",
    "        # Try candidate names\n",
    "        for var in var_candidates:\n",
    "            if var in f:\n",
    "                arr = np.array(f[var])\n",
    "                break\n",
    "        else:\n",
    "            # try to find a dataset with one dimension=24\n",
    "            arr = None\n",
    "            def _visit(name, obj):\n",
    "                nonlocal arr\n",
    "                if arr is not None:\n",
    "                    return\n",
    "                if hasattr(obj, \"shape\") and len(obj.shape) == 2 and (24 in obj.shape):\n",
    "                    arr = np.array(obj)\n",
    "            f.visititems(_visit)\n",
    "            if arr is None:\n",
    "                raise KeyError(\"No 2D dataset with a dimension of 24 found in the v7.3 file.\")\n",
    "\n",
    "        # MATLAB stores arrays column-major; h5py often yields transposed orientation.\n",
    "        # We want 24 x S\n",
    "        if arr.shape[0] != 24 and arr.shape[1] == 24:\n",
    "            arr = arr.T\n",
    "        if arr.shape[0] != 24:\n",
    "            raise ValueError(f\"Expected 24xS, got {arr.shape}\")\n",
    "        return arr.astype(float)\n",
    "\n",
    "def safe_makedirs(path: str):\n",
    "    os.makedirs(path, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13a8fd42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Load config (if available) ---\n",
    "cfg = None\n",
    "try:\n",
    "    import ro.params as ro_params\n",
    "    cfg = getattr(ro_params, PROB_NAME)\n",
    "    print(\"Loaded cfg from ro.params:\", cfg)\n",
    "except Exception as e:\n",
    "    print(\"Could not import ro.params / cfg. Using fallback paths only.\")\n",
    "    print(\"Reason:\", repr(e))\n",
    "\n",
    "# If cfg exists, prefer its data_path (some repos use ./data/, some use data/)\n",
    "if cfg is not None and hasattr(cfg, \"data_path\"):\n",
    "    # normalize to PROJECT_ROOT\n",
    "    cfg_data_path = cfg.data_path\n",
    "    # if cfg.data_path is relative, interpret under PROJECT_ROOT\n",
    "    DATA_ROOT = cfg_data_path if os.path.isabs(cfg_data_path) else os.path.join(PROJECT_ROOT, cfg_data_path)\n",
    "    PROB_DIR = os.path.join(DATA_ROOT, PROB_NAME)\n",
    "\n",
    "print(\"Resolved DATA_ROOT:\", DATA_ROOT)\n",
    "print(\"Resolved PROB_DIR:\", PROB_DIR)\n",
    "\n",
    "safe_makedirs(PROB_DIR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa2c8368",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Load generated ML dataset (ml_data*.pkl) ---\n",
    "# Typical filenames:\n",
    "#   ./data/offering_no_network/ml_data.pkl\n",
    "#   ./data/offering_no_network/ml_data_YYYYMMDD....pkl  (if you used --ml_suffix)\n",
    "ml_data_path = latest_file(os.path.join(PROB_DIR, \"ml_data*.pkl\"))\n",
    "print(\"Latest ml_data file:\", ml_data_path)\n",
    "\n",
    "if ml_data_path is None:\n",
    "    raise FileNotFoundError(\n",
    "        f\"No ml_data*.pkl found under {PROB_DIR}. \"\n",
    "        \"Run: python -m ro.scripts.02_generate_dataset --problem offering_no_network ...\"\n",
    "    )\n",
    "\n",
    "dataset = load_pkl(ml_data_path)\n",
    "\n",
    "# Expected: dict with keys like tr_data, val_data, maybe other metadata\n",
    "print(\"Dataset keys:\", list(dataset.keys()))\n",
    "print(\"Train samples:\", len(dataset.get(\"tr_data\", [])))\n",
    "print(\"Val samples:\", len(dataset.get(\"val_data\", [])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78267d5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Convert samples to a DataFrame for summary ---\n",
    "def flatten_samples(samples, split_name):\n",
    "    rows = []\n",
    "    for r in samples:\n",
    "        # DataManager stores these fields in result dict\n",
    "        inst = r.get(\"instance\", {})\n",
    "        rho = float(inst.get(\"rho\", 1.0))\n",
    "        fs = float(r.get(\"fs_obj\", np.nan))\n",
    "        ss = float(r.get(\"ss_obj\", np.nan))\n",
    "        # Two common conventions:\n",
    "        total_1 = fs + ss          # unweighted sum\n",
    "        total_2 = fs + rho * ss    # probability-weighted second-stage cost (matches Matlab MP scaling)\n",
    "        row = {\n",
    "            \"split\": split_name,\n",
    "            \"scenario_id\": int(inst.get(\"scenario_id\", -1)),\n",
    "            \"rho\": rho,\n",
    "            \"fs_obj\": fs,\n",
    "            \"ss_obj\": ss,\n",
    "            \"total_fs_plus_ss\": total_1,\n",
    "            \"total_fs_plus_rho_ss\": total_2,\n",
    "            \"time_s\": float(r.get(\"time\", np.nan)),\n",
    "        }\n",
    "        rows.append(row)\n",
    "    return rows\n",
    "\n",
    "rows = []\n",
    "rows += flatten_samples(dataset.get(\"tr_data\", []), \"train\")\n",
    "rows += flatten_samples(dataset.get(\"val_data\", []), \"val\")\n",
    "df = pd.DataFrame(rows)\n",
    "\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31deb3d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Summary stats ---\n",
    "def summarize(col):\n",
    "    s = df[col].dropna()\n",
    "    return pd.Series({\n",
    "        \"count\": int(s.shape[0]),\n",
    "        \"mean\": float(s.mean()) if len(s) else np.nan,\n",
    "        \"std\": float(s.std(ddof=1)) if len(s) > 1 else np.nan,\n",
    "        \"min\": float(s.min()) if len(s) else np.nan,\n",
    "        \"p25\": float(s.quantile(0.25)) if len(s) else np.nan,\n",
    "        \"median\": float(s.median()) if len(s) else np.nan,\n",
    "        \"p75\": float(s.quantile(0.75)) if len(s) else np.nan,\n",
    "        \"max\": float(s.max()) if len(s) else np.nan,\n",
    "    })\n",
    "\n",
    "summary = pd.DataFrame({\n",
    "    \"fs_obj\": summarize(\"fs_obj\"),\n",
    "    \"ss_obj\": summarize(\"ss_obj\"),\n",
    "    \"total_fs_plus_ss\": summarize(\"total_fs_plus_ss\"),\n",
    "    \"total_fs_plus_rho_ss\": summarize(\"total_fs_plus_rho_ss\"),\n",
    "    \"time_s\": summarize(\"time_s\"),\n",
    "}).T\n",
    "\n",
    "summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe2a21a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Per-scenario distribution (validation split) ---\n",
    "df_val = df[df[\"split\"] == \"val\"].copy()\n",
    "if df_val.empty:\n",
    "    print(\"No validation samples; showing all samples.\")\n",
    "    df_val = df.copy()\n",
    "\n",
    "by_s = df_val.groupby(\"scenario_id\").agg(\n",
    "    n=(\"scenario_id\", \"size\"),\n",
    "    mean_total=(\"total_fs_plus_rho_ss\", \"mean\"),\n",
    "    std_total=(\"total_fs_plus_rho_ss\", \"std\"),\n",
    "    mean_fs=(\"fs_obj\", \"mean\"),\n",
    "    mean_ss=(\"ss_obj\", \"mean\"),\n",
    ").reset_index().sort_values(\"scenario_id\")\n",
    "\n",
    "by_s.head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37955efd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Load and visualize the underlying instance profiles (price matrix, load, PV bounds) ---\n",
    "# 1) Locate price .mat path\n",
    "mat_path = None\n",
    "if cfg is not None and hasattr(cfg, \"price_mat_path\"):\n",
    "    mat_path = cfg.price_mat_path\n",
    "    if not os.path.isabs(mat_path):\n",
    "        mat_path = os.path.join(PROJECT_ROOT, mat_path)\n",
    "\n",
    "if mat_path is None:\n",
    "    # fallback: pick any .mat in problem dir\n",
    "    mat_path = latest_file(os.path.join(PROB_DIR, \"*.mat\"))\n",
    "\n",
    "print(\"Price matrix .mat path:\", mat_path)\n",
    "\n",
    "# 2) Load price matrix\n",
    "price_matrix = None\n",
    "if mat_path is not None and os.path.exists(mat_path):\n",
    "    price_matrix = try_load_price_matrix(mat_path)\n",
    "    print(\"price_matrix shape:\", price_matrix.shape)\n",
    "else:\n",
    "    print(\"No .mat price file found; skipping price matrix plots.\")\n",
    "\n",
    "# 3) Load one instance from dataset to pull load/PV bounds\n",
    "sample_any = None\n",
    "for split in (\"val\", \"train\"):\n",
    "    data = dataset.get(f\"{split}_data\", [])\n",
    "    if data:\n",
    "        sample_any = data[0]\n",
    "        break\n",
    "\n",
    "if sample_any is None:\n",
    "    raise RuntimeError(\"Dataset is empty.\")\n",
    "\n",
    "inst = sample_any[\"instance\"]\n",
    "T = int(inst.get(\"T\", 24))\n",
    "load = np.asarray(inst[\"p_load\"], dtype=float).reshape(-1)\n",
    "pv_min = np.asarray(inst[\"p_pv_min\"], dtype=float).reshape(-1)\n",
    "pv_max = np.asarray(inst[\"p_pv_max\"], dtype=float).reshape(-1)\n",
    "lam_rt = np.asarray(inst[\"lambda_rt\"], dtype=float).reshape(-1)\n",
    "Sbase = float(getattr(cfg, \"Sbase\", 1000.0)) if cfg is not None else 1000.0\n",
    "\n",
    "# --- plots ---\n",
    "t = np.arange(T)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(t, load, label=\"load (pu)\")\n",
    "ax.plot(t, pv_min, label=\"pv_min (pu)\")\n",
    "ax.plot(t, pv_max, label=\"pv_max (pu)\")\n",
    "ax.set_title(\"Load and PV bounds (per-unit)\")\n",
    "ax.set_xlabel(\"Hour\")\n",
    "ax.legend()\n",
    "plt.show()\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(t, lam_rt, label=\"lambda_rt\")\n",
    "ax.set_title(\"Real-time price (lambda_rt)\")\n",
    "ax.set_xlabel(\"Hour\")\n",
    "ax.legend()\n",
    "plt.show()\n",
    "\n",
    "if price_matrix is not None:\n",
    "    fig, ax = plt.subplots()\n",
    "    im = ax.imshow(price_matrix, aspect=\"auto\")\n",
    "    ax.set_title(\"Day-ahead price matrix (24 x S)\")\n",
    "    ax.set_xlabel(\"Scenario\")\n",
    "    ax.set_ylabel(\"Hour\")\n",
    "    plt.colorbar(im, ax=ax, label=\"DA price (raw units from .mat)\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adc93760",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Objective distributions ---\n",
    "cols = [\"fs_obj\", \"ss_obj\", \"total_fs_plus_ss\", \"total_fs_plus_rho_ss\"]\n",
    "for col in cols:\n",
    "    vals = df_val[col].dropna().values\n",
    "    if len(vals) == 0:\n",
    "        print(f\"Skip {col}: no values\")\n",
    "        continue\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.hist(vals, bins=30)\n",
    "    ax.set_title(f\"Validation distribution: {col}\")\n",
    "    ax.set_xlabel(col)\n",
    "    ax.set_ylabel(\"count\")\n",
    "    plt.show()\n",
    "\n",
    "# Boxplot across scenarios for total (validation)\n",
    "if df_val[\"scenario_id\"].nunique() > 1:\n",
    "    # keep scenarios ordered\n",
    "    scens = sorted(df_val[\"scenario_id\"].unique().tolist())\n",
    "    data = [df_val[df_val[\"scenario_id\"]==s][\"total_fs_plus_rho_ss\"].dropna().values for s in scens]\n",
    "    fig, ax = plt.subplots(figsize=(12,4))\n",
    "    ax.boxplot(data, labels=scens, showfliers=False)\n",
    "    ax.set_title(\"Validation total_fs_plus_rho_ss by scenario\")\n",
    "    ax.set_xlabel(\"scenario_id\")\n",
    "    ax.set_ylabel(\"total_fs_plus_rho_ss\")\n",
    "    plt.xticks(rotation=0)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1895fb07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Inspect one sample and (optionally) re-solve second-stage to visualize dispatch ---\n",
    "# This section needs gurobipy (and a working license). If unavailable, it will be skipped.\n",
    "\n",
    "sample = df_val.sample(1, random_state=0).iloc[0] if not df_val.empty else df.sample(1, random_state=0).iloc[0]\n",
    "# Find the actual record dict that matches this row (best-effort)\n",
    "def find_record(split, scenario_id):\n",
    "    for r in dataset.get(f\"{split}_data\", []):\n",
    "        inst = r.get(\"instance\", {})\n",
    "        if int(inst.get(\"scenario_id\", -999)) == int(scenario_id):\n",
    "            return r\n",
    "    return None\n",
    "\n",
    "rec = find_record(\"val\", int(sample[\"scenario_id\"])) or find_record(\"train\", int(sample[\"scenario_id\"]))\n",
    "if rec is None:\n",
    "    print(\"Could not find matching record; skipping solve/plot.\")\n",
    "else:\n",
    "    x  = np.asarray(rec[\"x\"], dtype=float).reshape(-1)\n",
    "    xi = np.asarray(rec[\"xi\"], dtype=float).reshape(-1)\n",
    "    inst = rec[\"instance\"]\n",
    "\n",
    "    print(\"scenario_id:\", inst.get(\"scenario_id\"))\n",
    "    print(\"fs_obj:\", rec.get(\"fs_obj\"), \"ss_obj:\", rec.get(\"ss_obj\"))\n",
    "    print(\"x range:\", float(x.min()), float(x.max()))\n",
    "    print(\"xi range:\", float(xi.min()), float(xi.max()))\n",
    "\n",
    "    # Try to solve with the OfferingNoNetwork class if available\n",
    "    try:\n",
    "        from ro.two_ro.offering_no_network import OfferingNoNetwork\n",
    "        two_ro = OfferingNoNetwork()\n",
    "        fs_obj, ss_obj, model = two_ro.solve_second_stage(x, xi, inst, verbose=0)\n",
    "        print(\"Re-solved: fs_obj=\", fs_obj, \"ss_obj=\", ss_obj)\n",
    "\n",
    "        # Extract trajectories\n",
    "        T = int(inst.get(\"T\", 24))\n",
    "        p_pv   = np.array([model._p_pv[t].X for t in range(T)])\n",
    "        p_ch   = np.array([model._p_ch[t].X for t in range(T)])\n",
    "        p_dis  = np.array([model._p_dis[t].X for t in range(T)])\n",
    "        soc    = np.array([model._soc[t].X for t in range(T)])\n",
    "        p_buy  = np.array([model._p_rt_b[t].X for t in range(T)])\n",
    "        p_sell = np.array([model._p_rt_s[t].X for t in range(T)])\n",
    "\n",
    "        t = np.arange(T)\n",
    "\n",
    "        fig, ax = plt.subplots()\n",
    "        ax.plot(t, x, label=\"p_DA (x)\")\n",
    "        ax.plot(t, p_pv, label=\"p_pv\")\n",
    "        ax.plot(t, p_dis - p_ch, label=\"p_dis - p_ch\")\n",
    "        ax.set_title(\"DA schedule and realized PV/ESS\")\n",
    "        ax.set_xlabel(\"Hour\")\n",
    "        ax.legend()\n",
    "        plt.show()\n",
    "\n",
    "        fig, ax = plt.subplots()\n",
    "        ax.plot(t, soc, label=\"SOC\")\n",
    "        ax.set_title(\"ESS State of Charge\")\n",
    "        ax.set_xlabel(\"Hour\")\n",
    "        ax.legend()\n",
    "        plt.show()\n",
    "\n",
    "        fig, ax = plt.subplots()\n",
    "        ax.plot(t, p_buy, label=\"RT buy\")\n",
    "        ax.plot(t, p_sell, label=\"RT sell\")\n",
    "        ax.set_title(\"RT deviation trades\")\n",
    "        ax.set_xlabel(\"Hour\")\n",
    "        ax.legend()\n",
    "        plt.show()\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"Skipping re-solve/plot due to error:\", repr(e))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a3d0fd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- If training artifacts exist: plot training curves from random_search ---\n",
    "# This looks for *tr_res*.pkl files produced by ro/scripts/03_train_model.py.\n",
    "\n",
    "rs_dir = os.path.join(PROB_DIR, \"random_search\")\n",
    "tr_res_path = latest_file(os.path.join(rs_dir, \"*_tr_res*.pkl\"))\n",
    "print(\"Latest training results:\", tr_res_path)\n",
    "\n",
    "if tr_res_path is None:\n",
    "    print(\"No training-results pickle found under:\", rs_dir)\n",
    "else:\n",
    "    tr_res = load_pkl(tr_res_path)\n",
    "    print(\"Training result keys:\", list(tr_res.keys()))\n",
    "    print(\"Best val_mae:\", tr_res.get(\"val_mae\"))\n",
    "    stats = tr_res.get(\"training_stats\", {})\n",
    "\n",
    "    # Plot losses if present\n",
    "    losses = stats.get(\"losses\", None)\n",
    "    if losses is not None and len(losses) > 0:\n",
    "        fig, ax = plt.subplots()\n",
    "        ax.plot(np.arange(1, len(losses)+1), losses)\n",
    "        ax.set_title(\"Training loss (mean per epoch)\")\n",
    "        ax.set_xlabel(\"Epoch\")\n",
    "        ax.set_ylabel(\"Loss\")\n",
    "        plt.show()\n",
    "\n",
    "    # Plot validation MAE trajectory if present\n",
    "    val_maes = stats.get(\"val_maes\", None)\n",
    "    if val_maes is not None and len(val_maes) > 0:\n",
    "        fig, ax = plt.subplots()\n",
    "        ax.plot(np.arange(1, len(val_maes)+1), val_maes)\n",
    "        ax.set_title(\"Validation MAE over evaluations\")\n",
    "        ax.set_xlabel(\"Eval step\")\n",
    "        ax.set_ylabel(\"MAE\")\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7b1204c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- If evaluation artifacts exist: summarize eval_results/*.pkl (best-effort) ---\n",
    "eval_dir = os.path.join(PROB_DIR, \"eval_results\")\n",
    "eval_files = sorted(glob.glob(os.path.join(eval_dir, \"*.pkl\")))\n",
    "print(\"Found eval_results files:\", len(eval_files))\n",
    "\n",
    "if not eval_files:\n",
    "    print(\"No eval_results/*.pkl found. If you ran evaluation scripts, check this folder:\", eval_dir)\n",
    "else:\n",
    "    rows = []\n",
    "    for fp in eval_files:\n",
    "        try:\n",
    "            d = load_pkl(fp)\n",
    "        except Exception:\n",
    "            continue\n",
    "\n",
    "        # Try common schemas used across Neur2RO notebooks\n",
    "        row = {\"file\": os.path.basename(fp)}\n",
    "        for k in [\"method\", \"time\", \"obj\", \"objective\", \"gap\", \"feasible\", \"n_iters\", \"val_mae\"]:\n",
    "            if k in d:\n",
    "                row[k] = d[k]\n",
    "        # Sometimes results stored under nested keys\n",
    "        if \"results\" in d and isinstance(d[\"results\"], dict):\n",
    "            for k in [\"time\", \"obj\", \"gap\", \"feasible\", \"n_iters\"]:\n",
    "                if k in d[\"results\"] and k not in row:\n",
    "                    row[k] = d[\"results\"][k]\n",
    "        rows.append(row)\n",
    "\n",
    "    df_eval = pd.DataFrame(rows)\n",
    "    display(df_eval.head(20))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
